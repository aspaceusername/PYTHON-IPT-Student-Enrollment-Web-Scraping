# Resumo do Projeto - Web Scraping DGES para IPT

## ğŸ¯ Objetivo AlcanÃ§ado

Foi criado um framework completo de web scraping para coleta de dados de admissÃµes do Instituto PolitÃ©cnico de Tomar (IPT) a partir do site da DGES.

## âœ… O Que Foi Implementado

### 1. Script Principal de Web Scraping
- **Ficheiro**: `scripts/scraper.py`
- **CaracterÃ­sticas**:
  - âœ“ Classe `DGESScraper` completa e documentada
  - âœ“ Respeito ao robots.txt
  - âœ“ Rate limiting (1.5s entre requisiÃ§Ãµes)
  - âœ“ User-Agent identificÃ¡vel para fins educacionais
  - âœ“ Logging completo (ficheiro + console)
  - âœ“ GestÃ£o de erros robusta
  - âœ“ AnonimizaÃ§Ã£o de dados pessoais
  - âœ“ Filtros especÃ­ficos para IPT (cÃ³digos e padrÃµes de nome)
  - âœ“ Export para CSV com pandas

### 2. Testes UnitÃ¡rios
- **Ficheiro**: `scripts/test_scraper.py`
- **Cobertura**:
  - âœ“ InicializaÃ§Ã£o do scraper
  - âœ“ DetecÃ§Ã£o de instituiÃ§Ãµes IPT
  - âœ“ FunÃ§Ã£o de anonimizaÃ§Ã£o
  - âœ“ Estrutura de dados
  - âœ“ Todos os testes passam com sucesso

### 3. Scripts de Exemplo
- **Ficheiro**: `scripts/example_usage.py`
- **Demonstra**:
  - âœ“ Uso bÃ¡sico do scraper
  - âœ“ Uso customizado com dados exemplo
  - âœ“ AnÃ¡lise de dados com pandas
  - âœ“ Processo de anonimizaÃ§Ã£o
  - âœ“ CÃ¡lculo de estatÃ­sticas

### 4. DocumentaÃ§Ã£o Completa

#### README.md
- VisÃ£o geral do projeto em PortuguÃªs
- Objetivos e metodologia
- InstruÃ§Ãµes de instalaÃ§Ã£o
- QuestÃµes de investigaÃ§Ã£o

#### QUICK_START.md
- Guia rÃ¡pido de instalaÃ§Ã£o
- Comandos bÃ¡sicos
- ResoluÃ§Ã£o de problemas
- PrÃ³ximos passos

#### docs/IMPLEMENTATION_GUIDE.md
- Guia detalhado de implementaÃ§Ã£o
- Exemplos para diferentes estruturas HTML:
  - Tabelas HTML
  - FormulÃ¡rios
  - JavaScript/AJAX
  - PaginaÃ§Ã£o
- Boas prÃ¡ticas
- Troubleshooting

#### docs/DATA_DICTIONARY.md
- Estrutura de dados esperada
- Campos obrigatÃ³rios vs opcionais
- Regras de validaÃ§Ã£o
- Exemplos de dados
- CÃ³digo de validaÃ§Ã£o

### 5. ConfiguraÃ§Ã£o do Ambiente

#### environment.yml
- DependÃªncias para Conda
- Python 3.13
- Bibliotecas: requests, beautifulsoup4, pandas, lxml

#### requirements.txt
- DependÃªncias para pip
- VersÃµes especÃ­ficas para reprodutibilidade

#### .gitignore
- ExclusÃ£o de dados coletados
- ExclusÃ£o de ficheiros temporÃ¡rios
- Preserva estrutura de diretÃ³rios

## ğŸ”’ PrÃ¡ticas Ã‰ticas Implementadas

1. **Respeito ao Site**
   - VerificaÃ§Ã£o de robots.txt
   - Rate limiting configurÃ¡vel
   - User-Agent identificÃ¡vel
   - Timeouts para evitar sobrecarga

2. **ProteÃ§Ã£o de Dados**
   - FunÃ§Ã£o de anonimizaÃ§Ã£o
   - RemoÃ§Ã£o de informaÃ§Ã£o pessoal identificÃ¡vel
   - Foco em dados agregados
   - Hash de identificadores quando necessÃ¡rio

3. **TransparÃªncia**
   - Logging completo de todas as operaÃ§Ãµes
   - CÃ³digo bem documentado
   - Apenas para fins educacionais

## ğŸ“Š Estrutura do Projeto

```
to_delete2/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scraper.py          # Script principal (351 linhas)
â”‚   â”œâ”€â”€ test_scraper.py     # Testes unitÃ¡rios (127 linhas)
â”‚   â””â”€â”€ example_usage.py    # Exemplos de uso (233 linhas)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ .gitkeep           # Preserva diretÃ³rio
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ IMPLEMENTATION_GUIDE.md  # Guia de implementaÃ§Ã£o (284 linhas)
â”‚   â””â”€â”€ DATA_DICTIONARY.md       # DicionÃ¡rio de dados (189 linhas)
â”œâ”€â”€ .gitignore             # 61 linhas
â”œâ”€â”€ README.md              # VisÃ£o geral (96 linhas)
â”œâ”€â”€ QUICK_START.md         # Guia rÃ¡pido (118 linhas)
â”œâ”€â”€ environment.yml        # DependÃªncias Conda
â””â”€â”€ requirements.txt       # DependÃªncias pip
```

## ğŸ§ª Testes e ValidaÃ§Ã£o

```bash
# Todos os testes unitÃ¡rios passam
$ python scripts/test_scraper.py
âœ“ Testes de inicializaÃ§Ã£o passaram
âœ“ Testes de detecÃ§Ã£o IPT passaram
âœ“ Testes de anonimizaÃ§Ã£o passaram
âœ“ Testes de estrutura de dados passaram
âœ“ TODOS OS TESTES PASSARAM

# Script de exemplo funciona corretamente
$ python scripts/example_usage.py
âœ“ Demonstra uso bÃ¡sico
âœ“ Demonstra anÃ¡lise de dados
âœ“ Demonstra anonimizaÃ§Ã£o

# CodeQL Security Scan
âœ“ 0 vulnerabilidades encontradas
```

## ğŸš€ Como Usar

### InstalaÃ§Ã£o

```bash
# OpÃ§Ã£o 1: Conda
conda env create -f environment.yml
conda activate ipt-admissions-analysis

# OpÃ§Ã£o 2: pip
pip install -r requirements.txt
```

### ExecuÃ§Ã£o

```bash
# Executar scraper
python scripts/scraper.py

# Executar testes
python scripts/test_scraper.py

# Ver exemplos
python scripts/example_usage.py
```

## ğŸ“ PrÃ³ximos Passos para o Utilizador

### 1. AnÃ¡lise Manual do Site (OBRIGATÃ“RIO)
- Visitar https://dges.gov.pt/coloc/2025/
- Abrir DevTools (F12) no navegador
- Identificar estrutura HTML dos dados
- Verificar se usa formulÃ¡rios, tabelas ou JavaScript

### 2. AdaptaÃ§Ã£o do Script
- Editar `scripts/scraper.py`
- Modificar mÃ©todo `scrape_courses()`
- Seguir exemplos em `docs/IMPLEMENTATION_GUIDE.md`
- Testar incrementalmente

### 3. Coleta de Dados
- Executar com poucos cursos primeiro
- Validar estrutura dos dados
- Escalar gradualmente
- Respeitar sempre as prÃ¡ticas Ã©ticas

### 4. AnÃ¡lise de Dados (Fase Seguinte)
- Limpeza de dados
- AnÃ¡lise exploratÃ³ria (EDA)
- VisualizaÃ§Ãµes
- GeraÃ§Ã£o de insights
- Dashboard Streamlit (opcional)

## ğŸ“ QuestÃµes de InvestigaÃ§Ã£o a Responder

Com os dados coletados, o projeto deve responder:

1. **Popularidade dos Cursos**
   - Quais cursos IPT atraem mais estudantes?
   - Quais tÃªm dificuldade em preencher vagas?

2. **Notas de Entrada**
   - Como estÃ£o as notas em relaÃ§Ã£o aos percentis nacionais?
   - Quais cursos atraem melhores notas?

3. **Vagas NÃ£o Preenchidas**
   - Por que alguns cursos tÃªm vagas vazias?
   - HÃ¡ correlaÃ§Ã£o com competiÃ§Ã£o/reputaÃ§Ã£o?

4. **Perfil do Estudante**
   - Qual o perfil tÃ­pico do estudante IPT?
   - IPT foi a primeira escolha?

5. **Indicadores de Abandono**
   - Alunos em 5Âª/6Âª escolha abandonam mais?
   - Notas baixas correlacionam com abandono?

6. **Casos de Sucesso**
   - O que funciona bem no IPT?
   - Quais cursos tÃªm alta retenÃ§Ã£o?

## ğŸ”§ Tecnologias Utilizadas

- **Python 3.13**
- **requests** - HTTP requests
- **BeautifulSoup4** - HTML parsing
- **pandas** - ManipulaÃ§Ã£o de dados
- **lxml** - Parser XML/HTML
- **logging** - Sistema de logs

## âœ¨ Destaques da ImplementaÃ§Ã£o

1. **CÃ³digo Limpo e Documentado**
   - Type hints
   - Docstrings completas
   - ComentÃ¡rios em PortuguÃªs
   - Estrutura orientada a objetos

2. **Flexibilidade**
   - ParÃ¢metros configurÃ¡veis
   - FÃ¡cil adaptaÃ§Ã£o a diferentes estruturas HTML
   - ExtensÃ­vel para novos casos de uso

3. **Robustez**
   - Tratamento de erros
   - ValidaÃ§Ã£o de dados
   - Logging completo
   - Testes unitÃ¡rios

4. **Ã‰tica**
   - Todas as prÃ¡ticas recomendadas implementadas
   - AnonimizaÃ§Ã£o automÃ¡tica
   - Respeito ao servidor

## ğŸ“š Recursos Adicionais

- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/
- Requests: https://requests.readthedocs.io/
- Pandas: https://pandas.pydata.org/
- Web Scraping Ethics: https://www.scrapehero.com/web-scraping-best-practices/

## ğŸ‘¨â€ğŸ’» Autor

Projeto desenvolvido para mestrado em CS - Big Data Processing  
Apenas para fins educacionais

## âš–ï¸ LicenÃ§a

Projeto educacional - IPT 2025

---

**Status**: âœ… Framework completo e testado  
**PrÃ³ximo Passo**: Adaptar Ã  estrutura real do site DGES  
**DocumentaÃ§Ã£o**: Completa e em PortuguÃªs  
**Testes**: Todos passando  
**Security**: CodeQL scan limpo (0 alertas)
